{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1G41xFrVdlzj-JQxhxZjmOJQ7Cl2JzrKq",
      "authorship_tag": "ABX9TyNo/k/AU+wB0B7eYtlHZ15L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKI123/OnlineChallengeStockMarket/blob/main/KerasTuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj9pCd7lmb3A",
        "outputId": "585a336a-c37c-465a-f148-cbe9ff4820a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYl49Y1RcLi5",
        "outputId": "f8f8bad2-ee90-4cf1-9945-2a91e2e69b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 16 Complete [00h 13m 27s]\n",
            "val_accuracy: 0.47502535581588745\n",
            "\n",
            "Best val_accuracy So Far: 0.4776473343372345\n",
            "Total elapsed time: 02h 57m 37s\n",
            "\n",
            "Search: Running Trial #17\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "140               |140               |units_lstm_1\n",
            "70                |70                |units_lstm_2\n",
            "0                 |0                 |dropout\n",
            "32                |32                |dense_units\n",
            "20                |7                 |tuner/epochs\n",
            "7                 |3                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "2                 |1                 |tuner/round\n",
            "0013              |0001              |tuner/trial_id\n",
            "\n",
            "Epoch 8/20\n",
            "18975/18975 [==============================] - 203s 10ms/step - loss: 0.9935 - accuracy: 0.4805 - val_loss: 0.9947 - val_accuracy: 0.4787 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "18975/18975 [==============================] - 200s 11ms/step - loss: 0.9917 - accuracy: 0.4817 - val_loss: 0.9949 - val_accuracy: 0.4811 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "18975/18975 [==============================] - 199s 11ms/step - loss: 0.9896 - accuracy: 0.4839 - val_loss: 0.9930 - val_accuracy: 0.4803 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "18975/18975 [==============================] - 200s 11ms/step - loss: 0.9876 - accuracy: 0.4852 - val_loss: 0.9915 - val_accuracy: 0.4831 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "18975/18975 [==============================] - 198s 10ms/step - loss: 0.9855 - accuracy: 0.4881 - val_loss: 0.9919 - val_accuracy: 0.4824 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "18975/18975 [==============================] - 198s 10ms/step - loss: 0.9837 - accuracy: 0.4900 - val_loss: 0.9911 - val_accuracy: 0.4831 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "18975/18975 [==============================] - 198s 10ms/step - loss: 0.9817 - accuracy: 0.4915 - val_loss: 0.9909 - val_accuracy: 0.4822 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "18975/18975 [==============================] - 197s 10ms/step - loss: 0.9796 - accuracy: 0.4934 - val_loss: 0.9912 - val_accuracy: 0.4827 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "18975/18975 [==============================] - 198s 10ms/step - loss: 0.9774 - accuracy: 0.4954 - val_loss: 0.9949 - val_accuracy: 0.4818 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "18975/18975 [==============================] - 198s 10ms/step - loss: 0.9750 - accuracy: 0.4974 - val_loss: 0.9928 - val_accuracy: 0.4812 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "18975/18975 [==============================] - 199s 10ms/step - loss: 0.9727 - accuracy: 0.4990 - val_loss: 0.9929 - val_accuracy: 0.4820 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "18975/18975 [==============================] - 201s 11ms/step - loss: 0.9705 - accuracy: 0.5014 - val_loss: 0.9943 - val_accuracy: 0.4814 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "  859/18975 [>.............................] - ETA: 2:41 - loss: 0.9581 - accuracy: 0.5092"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/input_training.csv', index_col=0)\n",
        "train_data_y = pd.read_csv('/content/drive/MyDrive/output_training_gmEd6Zt.csv', index_col=0)\n",
        "\n",
        "# Merge the input and output data on the 'ID' column\n",
        "merged_data = train_data.merge(train_data_y, left_index=True, right_index=True)\n",
        "\n",
        "# Scale the training data\n",
        "division_value = 100\n",
        "columns_to_transform = [f'r{i}' for i in range(53)]\n",
        "for column in columns_to_transform:\n",
        "    merged_data[column] = merged_data[column] / division_value\n",
        "merged_data[columns_to_transform] = np.tanh(merged_data[columns_to_transform])\n",
        "\n",
        "# Replace NaN values with the mask value\n",
        "mask_value = -2.0\n",
        "merged_data.fillna(mask_value, inplace=True)\n",
        "\n",
        "# Drop the 'day' and 'equity' columns from the DataFrame\n",
        "merged_data = merged_data.drop(['day', 'equity'], axis=1)\n",
        "\n",
        "# Map labels from [-1, 0, 1] to [0, 1, 2]\n",
        "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
        "merged_data['reod'] = merged_data['reod'].map(label_mapping)\n",
        "\n",
        "# Prepare the data\n",
        "X = merged_data.drop('reod', axis=1).values.reshape(-1, 53, 1)\n",
        "y = to_categorical(merged_data['reod'].values)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hp.Int('units_lstm_1', min_value=60, max_value=140, step=20), return_sequences=True, input_shape=(53, 1)))\n",
        "    model.add(LSTM(units=hp.Int('units_lstm_2', min_value=30, max_value=80, step=20), return_sequences=False))\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.05)))\n",
        "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=96, step=32), activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model, objective='val_accuracy', max_epochs=20, directory='keras_tuner_dir', project_name='lstm_optimization')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Execute the search\n",
        "tuner.search(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[reduce_lr])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the first LSTM layer is {best_hps.get('units_lstm_1')},\n",
        "the optimal number of units in the second LSTM layer is {best_hps.get('units_lstm_2')},\n",
        "the optimal dropout rate is {best_hps.get('dropout')}, and\n",
        "the optimal number of neurons in the first dense layer is {best_hps.get('dense_units')}.\n",
        "\"\"\")\n",
        "\n",
        "# Build the model with the optimal hyperparameters and train it on the data\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1, callbacks=[reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n"
      ]
    }
  ]
}